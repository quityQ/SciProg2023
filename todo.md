# To Do
- Set up external DB (PostgreSQL) using Amazon RDS
    - create db tables 
- Create python script to move data from web API to DB
- clean up data using the csv as a guide
    - clean up the array so they only have the relevant points
    - replace unavailable data with proper nulls
- Adapt jupyter notebook to use data from DB
    - use buckets to address extreme outliers


# Minimal Requirements
(1) Collection of real-world data
(2) Data preparation (e.g. strings to numerical using regular expressions)
(3) Use of Python built-in data structures (lists, dictionaries, sets, tuples) and pandas data frames
(4) Use of conditional statements, loop control statements and loops
(5) Use of procedural programming or object-oriented programming (one of these or both)
(6) Use of tables, vizualizations/graphics for data exploration
(7) Integration of a statistical analyses (e.g. correlation analysis, statistical tests)
(8) Making the data, Python code, Jupyter notebooks, etc. available on Moodle


# Extra Requirements
(1) Creativity of implementation (creative is everything which was not part of the lessons and exercises)
(2) Use of a Web API to collect data (must be part of the Python programming code) 
(3) Use of a database (e.g. SQLite, PostgreSQL) and SQL-queries (must be part of the Python programming code)
(4) Use of Docker or a cloud service (e.g. Amazon EC2, Amazon Sagemaker, Azure Machine Learning studio)
(5) Creation of a simple web application to present data and analysis results
(6) Making the Python code/Jupyter notebooks publicly available in a GitHub repo (exclude large data sets using .gitignore)